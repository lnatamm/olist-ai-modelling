{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise de Regressão - Olist Freight Value\n",
        "\n",
        "## Objetivo\n",
        "Prever o valor do frete (freight_value) usando diferentes modelos de regressão.\n",
        "\n",
        "## Requisitos Obrigatórios\n",
        "- ✅ Regressão Linear\n",
        "- ✅ PCA para redução de dimensionalidade  \n",
        "- ✅ 3 Métricas: R², MAE, RMSE\n",
        "\n",
        "## Modelos Comparados\n",
        "1. **Linear Regression** (obrigatório)\n",
        "2. **Random Forest Regressor**\n",
        "3. **Decision Tree Regressor**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dados carregados:\n"
          ]
        }
      ],
      "source": [
        "# Carregar datasets\n",
        "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
        "products = pd.read_csv('data/olist_products_dataset.csv')\n",
        "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
        "customers = pd.read_csv('data/olist_customers_dataset.csv')\n",
        "sellers = pd.read_csv('data/olist_sellers_dataset.csv')\n",
        "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv')\n",
        "\n",
        "print(\"Dados carregados:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features criadas\n"
          ]
        }
      ],
      "source": [
        "# Criar feature de volume do produto\n",
        "products['volume'] = (products['product_length_cm'] * \n",
        "                      products['product_height_cm'] * \n",
        "                      products['product_width_cm'])\n",
        "\n",
        "# Preencher valores ausentes na categoria\n",
        "products['product_category_name'] = products['product_category_name'].fillna('Indefinido')\n",
        "\n",
        "print(\"Features criadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Merge e Preparação dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dados unidos\n"
          ]
        }
      ],
      "source": [
        "# Merge dos dados\n",
        "data = (order_items\n",
        "        .merge(products, on='product_id')\n",
        "        .merge(orders, on='order_id')\n",
        "        .merge(customers, on='customer_id')\n",
        "        .merge(sellers, on='seller_id')\n",
        "        .merge(reviews, on='order_id'))\n",
        "\n",
        "print(\"Dados unidos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Criação de Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features criadas: ['volume', 'product_weight_g', 'customer_state', 'seller_state', 'same_state', 'product_category']\n",
            "Target (Frete) - média: R$ 19.99\n"
          ]
        }
      ],
      "source": [
        "# Criar feature binária: mesmo estado?\n",
        "data['same_state'] = (data['customer_state'] == data['seller_state']).astype(int)\n",
        "\n",
        "# Codificar variáveis categóricas\n",
        "data['customer_state'] = data['customer_state'].astype('category').cat.codes\n",
        "data['seller_state'] = data['seller_state'].astype('category').cat.codes\n",
        "data['product_category'] = data['product_category_name'].astype('category').cat.codes\n",
        "\n",
        "# Definir features para o modelo\n",
        "features = [\n",
        "    'volume', \n",
        "    'product_weight_g', \n",
        "    'customer_state', \n",
        "    'seller_state', \n",
        "    'same_state', \n",
        "    'product_category'\n",
        "]\n",
        "\n",
        "# Preparar X e y (removendo NaN)\n",
        "X = data[features].dropna()\n",
        "y = data.loc[X.index, 'freight_value']\n",
        "\n",
        "print(f\"Features criadas: {features}\")\n",
        "print(f\"Target (Frete) - média: R$ {y.mean():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train/Test Split e Normalização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dividir em treino e teste (80/20)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train, X_test, y_train, y_test = train_test_split(\u001b[43mX\u001b[49m, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Normalizar os dados (necessário para PCA)\u001b[39;00m\n\u001b[32m      5\u001b[39m scaler = StandardScaler()\n",
            "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# Dividir em treino e teste (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar os dados (necessário para PCA)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Divisão dos dados (20/80)\")\n",
        "print(\"Normalização aplicada com StandardScaler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PCA - Redução de Dimensionalidade (Obrigatório)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_train_scaled' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Aplicar PCA mantendo 95% da variância\u001b[39;00m\n\u001b[32m      2\u001b[39m pca = PCA(n_components=\u001b[32m0.95\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m X_train_pca = pca.fit_transform(\u001b[43mX_train_scaled\u001b[49m)\n\u001b[32m      4\u001b[39m X_test_pca = pca.transform(X_test_scaled)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPCA aplicado:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'X_train_scaled' is not defined"
          ]
        }
      ],
      "source": [
        "# Aplicar PCA mantendo 95% da variância\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"PCA aplicado:\")\n",
        "print(f\"Dimensões originais: {X_train_scaled.shape[1]}\")\n",
        "print(f\"Dimensões após PCA: {X_train_pca.shape[1]}\")\n",
        "print(f\"\\nComponentes principais mantidos: {len(pca.explained_variance_ratio_)}\")\n",
        "print(f\"Features originais: {features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Definição dos Modelos de Regressão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelos definidos:\n",
            "   1. Linear Regression\n",
            "   2. Random Forest\n",
            "   3. Decision Tree\n"
          ]
        }
      ],
      "source": [
        "# Definir os 3 modelos para comparação\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(\n",
        "        n_estimators=100, \n",
        "        max_depth=15, \n",
        "        min_samples_leaf=2, \n",
        "        max_features='sqrt', \n",
        "        random_state=42, \n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Decision Tree': DecisionTreeRegressor(\n",
        "        max_depth=15, \n",
        "        min_samples_leaf=2, \n",
        "        ccp_alpha=0.0, \n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"Modelos definidos:\")\n",
        "for i, model_name in enumerate(models.keys(), 1):\n",
        "    print(f\"   {i}. {model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Treinamento e Avaliação dos Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Modelo: Linear Regression\n",
            "============================================================\n",
            "R² Score: 0.5080\n",
            "MAE (Mean Absolute Error): R$ 5.96\n",
            "RMSE (Root Mean Squared Error): R$ 11.15\n",
            "Predições dentro de ±20%: 48.93%\n",
            "\n",
            "============================================================\n",
            "Modelo: Random Forest\n",
            "============================================================\n",
            "R² Score: 0.6911\n",
            "MAE (Mean Absolute Error): R$ 4.26\n",
            "RMSE (Root Mean Squared Error): R$ 8.84\n",
            "Predições dentro de ±20%: 67.63%\n",
            "\n",
            "============================================================\n",
            "Modelo: Decision Tree\n",
            "============================================================\n",
            "R² Score: 0.5880\n",
            "MAE (Mean Absolute Error): R$ 4.69\n",
            "RMSE (Root Mean Squared Error): R$ 10.20\n",
            "Predições dentro de ±20%: 66.25%\n",
            "\n",
            "Todos os modelos foram treinados e avaliados!\n"
          ]
        }
      ],
      "source": [
        "# Dicionário para armazenar resultados\n",
        "results = {}\n",
        "\n",
        "# Treinar e avaliar cada modelo\n",
        "for model_name, model in models.items():\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Modelo: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Treinar modelo\n",
        "    model.fit(X_train_pca, y_train)\n",
        "    \n",
        "    # Predições\n",
        "    y_pred = model.predict(X_test_pca)\n",
        "    \n",
        "    # Calcular as 3 métricas obrigatórias\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "    \n",
        "    # Calcular acurácia com tolerância\n",
        "    tolerance = 0.2  # 20% de tolerância\n",
        "    accuracy = (abs(y_test - y_pred) / y_test <= tolerance).mean() * 100\n",
        "    \n",
        "    # Armazenar resultados\n",
        "    results[model_name] = {\n",
        "        'R²': r2,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'Acurácia (±20%)': accuracy\n",
        "    }\n",
        "    \n",
        "    # Exibir métricas\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"MAE (Mean Absolute Error): R$ {mae:.2f}\")\n",
        "    print(f\"RMSE (Root Mean Squared Error): R$ {rmse:.2f}\")\n",
        "    print(f\"Predições dentro de ±20%: {accuracy:.2f}%\")\n",
        "    print()\n",
        "\n",
        "print(\"Todos os modelos foram treinados e avaliados!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Comparação dos Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "COMPARAÇÃO DOS MODELOS\n",
            "======================================================================\n",
            "Modelo                    R²         MAE          RMSE         Acurácia  \n",
            "----------------------------------------------------------------------\n",
            "Linear Regression         0.5080     R$ 5.96      R$ 11.15     48.93    %\n",
            "Random Forest             0.6911     R$ 4.26      R$ 8.84      67.63    %\n",
            "Decision Tree             0.5880     R$ 4.69      R$ 10.20     66.25    %\n",
            "\n",
            "======================================================================\n",
            "Melhor modelo (por R²): Random Forest\n",
            "   - R² Score: 0.6911\n",
            "   - MAE: R$ 4.26\n",
            "   - RMSE: R$ 8.84\n",
            "   - Acurácia (±20%): 67.63%\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Tabela comparativa dos modelos\n",
        "print(f\"{'='*70}\")\n",
        "print(\"COMPARAÇÃO DOS MODELOS\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"{'Modelo':<25} {'R²':<10} {'MAE':<12} {'RMSE':<12} {'Acurácia':<10}\")\n",
        "print(f\"{'-'*70}\")\n",
        "\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name:<25} {metrics['R²']:<10.4f} R$ {metrics['MAE']:<9.2f} R$ {metrics['RMSE']:<9.2f} {metrics['Acurácia (±20%)']:<9.2f}%\")\n",
        "\n",
        "# Identificar o melhor modelo baseado em R²\n",
        "best_model = max(results.items(), key=lambda x: x[1]['R²'])\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Melhor modelo (por R²): {best_model[0]}\")\n",
        "print(f\"   - R² Score: {best_model[1]['R²']:.4f}\")\n",
        "print(f\"   - MAE: R$ {best_model[1]['MAE']:.2f}\")\n",
        "print(f\"   - RMSE: R$ {best_model[1]['RMSE']:.2f}\")\n",
        "print(f\"   - Acurácia (±20%): {best_model[1]['Acurácia (±20%)']:.2f}%\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10. Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features criadas\n"
          ]
        }
      ],
      "source": [
        "# Criar feature de volume do produto\n",
        "data['time_to_deliver'] = (pd.to_datetime(data['order_delivered_customer_date']) - pd.to_datetime(data['order_purchase_timestamp'])).dt.days\n",
        "\n",
        "print(\"Features criadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "11. Criação das Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features criadas: ['price', 'freight_value']\n",
            "Target (Review Score) - média: 4.03\n"
          ]
        }
      ],
      "source": [
        "# Definir features para o modelo\n",
        "features = [\n",
        "    'price',\n",
        "    'freight_value'\n",
        "]\n",
        "\n",
        "# Preparar X e y (removendo NaN)\n",
        "X = data[features].dropna()\n",
        "y = data.loc[X.index, 'review_score']\n",
        "\n",
        "print(f\"Features criadas: {features}\")\n",
        "print(f\"Target (Review Score) - média: {y.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "12. Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Divisão dos dados (20/80)\n",
            "Normalização aplicada com StandardScaler\n"
          ]
        }
      ],
      "source": [
        "# Dividir em treino e teste (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar os dados (necessário para PCA)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Divisão dos dados (20/80)\")\n",
        "print(\"Normalização aplicada com StandardScaler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "13. PCA - Redução de Dimensionalidade (Obrigatório)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PCA aplicado:\n",
            "Dimensões originais: 2\n",
            "Dimensões após PCA: 2\n",
            "\n",
            "Componentes principais mantidos: 2\n",
            "Features originais: ['price', 'freight_value']\n"
          ]
        }
      ],
      "source": [
        "# Aplicar PCA mantendo 95% da variância\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"PCA aplicado:\")\n",
        "print(f\"Dimensões originais: {X_train_scaled.shape[1]}\")\n",
        "print(f\"Dimensões após PCA: {X_train_pca.shape[1]}\")\n",
        "print(f\"\\nComponentes principais mantidos: {len(pca.explained_variance_ratio_)}\")\n",
        "print(f\"Features originais: {features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "14. Definição dos Modelos de Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelos definidos:\n",
            "   1. K-Means\n",
            "   2. DBSCAN\n",
            "   3. Agglomerative Clustering\n"
          ]
        }
      ],
      "source": [
        "# Definir os 3 modelos para comparação\n",
        "models = {\n",
        "    'K-Means': KMeans(\n",
        "        init='k-means++',\n",
        "        n_clusters=4,\n",
        "    ),\n",
        "    'DBSCAN': DBSCAN(\n",
        "        eps=0.8,\n",
        "        min_samples=5\n",
        "    ),\n",
        "    'Agglomerative Clustering': AgglomerativeClustering(\n",
        "        n_clusters=4,\n",
        "        linkage='ward'\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"Modelos definidos:\")\n",
        "for i, model_name in enumerate(models.keys(), 1):\n",
        "    print(f\"   {i}. {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "15. Treinamento e Avaliação dos Modelos de Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Modelo: K-Means\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_clusters > \u001b[32m1\u001b[39m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         sil_score = \u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilhouette_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m         ch_score = metrics.calinski_harabasz_score(X_train_pca, labels)\n\u001b[32m     28\u001b[39m         db_score = metrics.davies_bouldin_score(X_train_pca, labels)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:138\u001b[39m, in \u001b[36msilhouette_score\u001b[39m\u001b[34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[39m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    137\u001b[39m         X, labels = X[indices], labels[indices]\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np.mean(\u001b[43msilhouette_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:302\u001b[39m, in \u001b[36msilhouette_samples\u001b[39m\u001b[34m(X, labels, metric, **kwds)\u001b[39m\n\u001b[32m    298\u001b[39m kwds[\u001b[33m\"\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m\"\u001b[39m] = metric\n\u001b[32m    299\u001b[39m reduce_func = functools.partial(\n\u001b[32m    300\u001b[39m     _silhouette_reduce, labels=labels, label_freqs=label_freqs\n\u001b[32m    301\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m results = \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m intra_clust_dists, inter_clust_dists = results\n\u001b[32m    304\u001b[39m intra_clust_dists = np.concatenate(intra_clust_dists)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2240\u001b[39m, in \u001b[36mpairwise_distances_chunked\u001b[39m\u001b[34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[39m\n\u001b[32m   2238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2239\u001b[39m     X_chunk = X[sl]\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m D_chunk = \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[32m   2242\u001b[39m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2243\u001b[39m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[32m   2244\u001b[39m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[32m   2245\u001b[39m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[32m   2246\u001b[39m     D_chunk.flat[sl.start :: _num_samples(X) + \u001b[32m1\u001b[39m] = \u001b[32m0\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2476\u001b[39m, in \u001b[36mpairwise_distances\u001b[39m\u001b[34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[39m\n\u001b[32m   2473\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m distance.squareform(distance.pdist(X, metric=metric, **kwds))\n\u001b[32m   2474\u001b[39m     func = partial(distance.cdist, metric=metric, **kwds)\n\u001b[32m-> \u001b[39m\u001b[32m2476\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1960\u001b[39m, in \u001b[36m_parallel_pairwise\u001b[39m\u001b[34m(X, Y, func, n_jobs, **kwds)\u001b[39m\n\u001b[32m   1957\u001b[39m X, Y, dtype = _return_float_dtype(X, Y)\n\u001b[32m   1959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[32m   1963\u001b[39m fd = delayed(_dist_wrapper)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:388\u001b[39m, in \u001b[36meuclidean_distances\u001b[39m\u001b[34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[39m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared.shape != (\u001b[32m1\u001b[39m, Y.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m    383\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    384\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    385\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    386\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:424\u001b[39m, in \u001b[36m_euclidean_distances\u001b[39m\u001b[34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[39m\n\u001b[32m    421\u001b[39m     distances = _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     distances = -\u001b[32m2\u001b[39m * \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m     distances += XX\n\u001b[32m    426\u001b[39m     distances += YY\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:206\u001b[39m, in \u001b[36msafe_sparse_dot\u001b[39m\u001b[34m(a, b, dense_output)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    203\u001b[39m     ret = a @ b\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[43msparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m sparse.issparse(b)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[32m    209\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[33m\"\u001b[39m\u001b[33mtoarray\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    210\u001b[39m ):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.toarray()\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lnata\\Projects\\olist-ai-modelling\\venv\\Lib\\site-packages\\scipy\\_lib\\_sparse.py:10\u001b[39m, in \u001b[36missparse\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSparseABC\u001b[39;00m(ABC):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34missparse\u001b[39m(x):\n\u001b[32m     11\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m \u001b[33;03m    False\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, SparseABC)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Treinar e avaliar modelos de clustering\n",
        "# Usando amostra de 10.000 pontos para reduzir tempo e memória\n",
        "sample_size = 10000\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(len(X_train_pca), size=min(sample_size, len(X_train_pca)), replace=False)\n",
        "X_train_sample = X_train_pca[sample_indices]\n",
        "\n",
        "cluster_results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Modelo: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Treinar e obter rótulos na amostra\n",
        "    labels = model.fit_predict(X_train_sample)\n",
        "    \n",
        "    # Informações de clusters\n",
        "    label_counts = dict(Counter(labels))\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    noise_count = int(label_counts.get(-1, 0))\n",
        "    \n",
        "    # Métricas (só calculáveis se houver pelo menos 2 clusters)\n",
        "    sil_score = None\n",
        "    ch_score = None\n",
        "    db_score = None\n",
        "    if n_clusters > 1:\n",
        "        try:\n",
        "            sil_score = metrics.silhouette_score(X_train_sample, labels)\n",
        "            ch_score = metrics.calinski_harabasz_score(X_train_sample, labels)\n",
        "            db_score = metrics.davies_bouldin_score(X_train_sample, labels)\n",
        "        except Exception as e:\n",
        "            # caso raro de erro numérico\n",
        "            sil_score = ch_score = db_score = None\n",
        "    \n",
        "    # Inércia (apenas KMeans possui)\n",
        "    inertia = getattr(model, 'inertia_', None)\n",
        "    \n",
        "    # Armazenar resultados\n",
        "    cluster_results[model_name] = {\n",
        "        'n_clusters': n_clusters,\n",
        "        'cluster_counts': label_counts,\n",
        "        'noise_count': noise_count,\n",
        "        'inertia': float(inertia) if inertia is not None else None,\n",
        "        'silhouette': float(sil_score) if sil_score is not None else None,\n",
        "        'calinski_harabasz': float(ch_score) if ch_score is not None else None,\n",
        "        'davies_bouldin': float(db_score) if db_score is not None else None,\n",
        "        'labels_sample': labels[:10].tolist()  # amostra dos rótulos\n",
        "    }\n",
        "    \n",
        "    # Exibir métricas resumidas\n",
        "    print(f\"Clusters encontrados (excluindo ruído): {n_clusters}\")\n",
        "    print(f\"Contagem por rótulo (inclui -1 para ruído): {label_counts}\")\n",
        "    if inertia is not None:\n",
        "        print(f\"Inertia: {inertia:.4f}\")\n",
        "    print(f\"Silhouette: {sil_score if sil_score is not None else 'N/A'}\")\n",
        "    print(f\"Calinski-Harabasz: {ch_score if ch_score is not None else 'N/A'}\")\n",
        "    print(f\"Davies-Bouldin: {db_score if db_score is not None else 'N/A'}\")\n",
        "    print()\n",
        "\n",
        "print(f\"Avaliação de clustering concluída (amostra de {len(X_train_sample):,} pontos).\")\n",
        "# resultados disponíveis em `cluster_results`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Visualização dos Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processando K-Means...\n",
            "Processando DBSCAN...\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Criar amostra para visualização\n",
        "# sample_size = 10000\n",
        "# np.random.seed(42)\n",
        "# sample_indices = np.random.choice(len(X_train_pca), size=min(sample_size, len(X_train_pca)), replace=False)\n",
        "X_sample = X_train_pca#[sample_indices]\n",
        "\n",
        "# Configurar figura com subplots para os modelos\n",
        "n_models = len(models)\n",
        "fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "fig.suptitle(f'Visualização dos Clusters - Features: Price + Freight Value (Amostra de {len(X_sample):,} pontos)', fontsize=16, y=1.02)\n",
        "\n",
        "# Cores para clusters (incluindo ruído como cinza)\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E2']\n",
        "noise_color = '#808080'\n",
        "\n",
        "for idx, (model_name, model) in enumerate(models.items()):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    print(f\"Processando {model_name}...\")\n",
        "    \n",
        "    # Obter labels do clustering na amostra\n",
        "    labels = model.fit_predict(X_sample)\n",
        "    \n",
        "    # Plotar pontos\n",
        "    unique_labels = set(labels)\n",
        "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
        "    \n",
        "    for label in unique_labels:\n",
        "        if label == -1:\n",
        "            # Ruído (DBSCAN)\n",
        "            color = noise_color\n",
        "            marker = 'x'\n",
        "            label_name = 'Ruído'\n",
        "            alpha = 0.3\n",
        "        else:\n",
        "            color = colors[label % len(colors)]\n",
        "            marker = 'o'\n",
        "            label_name = f'Cluster {label}'\n",
        "            alpha = 0.6\n",
        "        \n",
        "        mask = labels == label\n",
        "        ax.scatter(\n",
        "            X_sample[mask, 0], \n",
        "            X_sample[mask, 1] if X_sample.shape[1] > 1 else X_sample[mask, 0],\n",
        "            c=color,\n",
        "            label=label_name,\n",
        "            alpha=alpha,\n",
        "            edgecolors='black',\n",
        "            linewidth=0.5,\n",
        "            marker=marker,\n",
        "            s=30\n",
        "        )\n",
        "    \n",
        "    # Configurar subplot\n",
        "    ax.set_title(f'{model_name}\\n({n_clusters} clusters)', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('PCA Component 1 (Price)', fontsize=10)\n",
        "    ax.set_ylabel('PCA Component 2 (Freight)' if X_sample.shape[1] > 1 else 'PCA Component 1', fontsize=10)\n",
        "    ax.legend(loc='upper right', fontsize=8, framealpha=0.9)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisualizações geradas para todos os modelos de clustering\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
